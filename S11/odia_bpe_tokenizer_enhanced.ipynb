{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Byte Pair Encoding for Odia Text\n",
    "\n",
    "## Input Data Structure\n",
    "```\n",
    "project_root/\n",
    "├── odia_bpe_tokenizer_enhanced.ipynb\n",
    "├── data/\n",
    "│   └── odia/\n",
    "│       ├── file1.txt\n",
    "│       └── file2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    pre {\n",
       "        white-space: pre-wrap;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Define Odia patterns\n",
    "ODIA_PATTERNS = {\n",
    "    'basic': re.compile(r\"\"\"\n",
    "        \\s*[\\u0B00-\\u0B7F]+  # Basic Odia characters\n",
    "        |\\s*[\\p{N}]+         # Numbers\n",
    "        |\\s*[^\\s\\p{L}\\p{N}]+ # Punctuation and symbols\n",
    "        |\\s+                 # Whitespace\n",
    "        \"\"\", re.VERBOSE),\n",
    "    \n",
    "    'linguistic': re.compile(r\"\"\"\n",
    "        # Consonant clusters with virama\n",
    "        \\s*[\\u0B15-\\u0B39]\\u0B4D[\\u0B15-\\u0B39]+  \n",
    "        # CV combinations\n",
    "        |\\s*[\\u0B15-\\u0B39][\\u0B3E-\\u0B4C]?      \n",
    "        # Independent vowels\n",
    "        |\\s*[\\u0B05-\\u0B14]                       \n",
    "        # Numbers and punctuation\n",
    "        |\\s*[\\p{N}]+                              \n",
    "        |\\s*[^\\s\\p{L}\\p{N}]+\n",
    "        |\\s+\n",
    "        \"\"\", re.VERBOSE)\n",
    "}\n",
    "\n",
    "# Fix for CSS display\n",
    "display(HTML('''\n",
    "<style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedOdiaTokenizer:\n",
    "    def __init__(self, \n",
    "                 max_vocab_size: int = 16000,\n",
    "                 target_compression: float = 4.0,\n",
    "                 max_token_length: int = 24,\n",
    "                 pattern_type: str = 'linguistic',\n",
    "                 max_tokens_per_sequence: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Odia BPE tokenizer\n",
    "        \n",
    "        Args:\n",
    "            max_vocab_size: Maximum vocabulary size (default 16000)\n",
    "            target_compression: Target compression ratio (default 4.0)\n",
    "            max_token_length: Maximum token length in characters (default 24)\n",
    "            pattern_type: Type of tokenization pattern ('basic', 'detailed', or 'linguistic')\n",
    "            max_tokens_per_sequence: Maximum tokens per sequence (default None)\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.target_compression = target_compression\n",
    "        self.max_token_length = max_token_length\n",
    "        self.pattern = ODIA_PATTERNS.get(pattern_type, ODIA_PATTERNS['basic'])\n",
    "        \n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            '<UNK>': 0,  # Unknown token\n",
    "            '<S>': 1,    # Start of text\n",
    "            '</S>': 2    # End of text\n",
    "        }\n",
    "        \n",
    "        # Vocabulary mappings\n",
    "        self.stoi: Dict[str, int] = {}  # String to index\n",
    "        self.itos: Dict[int, str] = {}  # Index to string\n",
    "        self.merges: Dict[Tuple[int, int], int] = {}  # Merge rules\n",
    "        \n",
    "    def _is_odia_char(self, char: str) -> bool:\n",
    "        \"\"\"Check if character is in Odia Unicode range\"\"\"\n",
    "        return '\\u0B00' <= char <= '\\u0B7F'\n",
    "    \n",
    "    def _calculate_compression(self, text: str, tokens: List[int]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate compression ratio\n",
    "        \n",
    "        Args:\n",
    "            text: Original text\n",
    "            tokens: List of token indices\n",
    "            \n",
    "        Returns:\n",
    "            Compression ratio (original size / tokenized size)\n",
    "        \"\"\"\n",
    "        original_size = len(text.encode('utf-8'))\n",
    "        bits_per_token = np.ceil(np.log2(len(self.stoi)))\n",
    "        tokenized_size = len(tokens) * np.ceil(bits_per_token / 8)  # Convert bits to bytes\n",
    "        return original_size / tokenized_size\n",
    "\n",
    "    def _get_merge_score(self, pair: Tuple[int, int], freq: int, text_len: int) -> float:\n",
    "        \"\"\"Enhanced scoring with more aggressive merging\"\"\"\n",
    "        def get_str(p):\n",
    "            if isinstance(p, int):\n",
    "                token = self.itos[p]\n",
    "                if isinstance(token, tuple):\n",
    "                    return ''.join(get_str(t) for t in token)\n",
    "                return token\n",
    "            return str(p)\n",
    "        \n",
    "        token_str = ''.join(get_str(p) for p in pair)\n",
    "        token_len = len(token_str)\n",
    "        \n",
    "        # More relaxed length constraint\n",
    "        if token_len > self.max_token_length:\n",
    "            return 0.0\n",
    "            \n",
    "        # Enhanced scoring system with higher weights\n",
    "        length_bonus = np.log2(token_len + 1) * 2.0  # Doubled length bonus\n",
    "        freq_score = (freq / text_len) * 1.5         # Increased frequency weight\n",
    "        \n",
    "        # Enhanced linguistic bonuses\n",
    "        is_odia_cluster = any(self._is_odia_char(c) for c in token_str)\n",
    "        has_conjunct = '\\u0B4D' in token_str\n",
    "        has_matra = any(c in token_str for c in '\\u0B3E\\u0B3F\\u0B40\\u0B41\\u0B42\\u0B43\\u0B47\\u0B48\\u0B4B\\u0B4C')\n",
    "        \n",
    "        # More aggressive linguistic scoring\n",
    "        linguistic_score = 1.0\n",
    "        if is_odia_cluster:\n",
    "            linguistic_score *= 2.0        # Increased from 1.5\n",
    "        if has_conjunct:\n",
    "            linguistic_score *= 1.8        # Increased from 1.3\n",
    "        if has_matra:\n",
    "            linguistic_score *= 1.5        # Increased from 1.2\n",
    "            \n",
    "        # Additional bonuses for common patterns\n",
    "        if self._is_common_word_part(token_str):\n",
    "            linguistic_score *= 1.5\n",
    "            \n",
    "        return freq_score * length_bonus * linguistic_score\n",
    "\n",
    "    def _is_common_word_part(self, token_str: str) -> bool:\n",
    "        \"\"\"Check if token is likely to be a meaningful word part\"\"\"\n",
    "        # Common Odia word endings\n",
    "        common_endings = [\n",
    "            'ର', 'ରେ', 'ଟି', 'ଗୁଡ଼ିକ', 'ମାନେ', 'ଙ୍କୁ', 'ଙ୍କ', 'ଙ୍କର',\n",
    "            'ଟା', 'ଟାରେ', 'ଗୁଡ଼ାକ', 'ମାନଙ୍କ', 'ମାନଙ୍କୁ'\n",
    "        ]\n",
    "        \n",
    "        # Common Odia word beginnings\n",
    "        common_beginnings = [\n",
    "            'ପ୍ର', 'ଅନୁ', 'ଅଧି', 'ପରି', 'ଉପ', 'ସମ', 'ବି', 'ନି', 'ସୁ',\n",
    "            'ଆ', 'ଇ', 'ଉ', 'ଏ', 'ଓ'\n",
    "        ]\n",
    "        \n",
    "        return (any(token_str.endswith(end) for end in common_endings) or\n",
    "                any(token_str.startswith(begin) for begin in common_beginnings))\n",
    "\n",
    "    def _get_stats(self, ids: List[int]) -> Dict[Tuple[int, int], int]:\n",
    "        \"\"\"Count token pair frequencies\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        for pair in zip(ids, ids[1:]):\n",
    "            stats[pair] += 1\n",
    "        return stats\n",
    "\n",
    "    def _merge(self, ids: List[int], pair: Tuple[int, int], idx: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Merge all occurrences of pair into a new token\n",
    "        \n",
    "        Args:\n",
    "            ids: List of token indices\n",
    "            pair: Pair to merge\n",
    "            idx: New token index\n",
    "            \n",
    "        Returns:\n",
    "            Updated list of token indices\n",
    "        \"\"\"\n",
    "        newids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                newids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                newids.append(ids[i])\n",
    "                i += 1\n",
    "        return newids\n",
    "\n",
    "    def pre_tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Pre-tokenize text using regex pattern\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            List of pre-tokenized strings\n",
    "        \"\"\"\n",
    "        tokens = self.pattern.findall(text)\n",
    "        return [t.strip() for t in tokens if t.strip()]\n",
    "\n",
    "    def train(self, text: str) -> float:\n",
    "        \"\"\"Modified train method with more aggressive merging\"\"\"\n",
    "        # Pre-tokenize with linguistic patterns\n",
    "        pre_tokens = self.pre_tokenize(text)\n",
    "        processed_text = ' '.join(pre_tokens)\n",
    "        \n",
    "        # Initialize with more combinations\n",
    "        chars = sorted(list(set(processed_text)))\n",
    "        initial_tokens = chars.copy()\n",
    "        \n",
    "        # Add more initial combinations\n",
    "        for i, char in enumerate(chars):\n",
    "            if self._is_odia_char(char):\n",
    "                # Add all possible vowel mark combinations\n",
    "                for matra in '\\u0B3E\\u0B3F\\u0B40\\u0B41\\u0B42\\u0B43\\u0B47\\u0B48\\u0B4B\\u0B4C':\n",
    "                    combined = char + matra\n",
    "                    initial_tokens.append(combined)\n",
    "                \n",
    "                # Add more consonant clusters\n",
    "                for j, next_char in enumerate(chars):\n",
    "                    if self._is_odia_char(next_char):\n",
    "                        # Single conjunct\n",
    "                        cluster = char + '\\u0B4D' + next_char\n",
    "                        initial_tokens.append(cluster)\n",
    "                        \n",
    "                        # Double conjuncts (common in Odia)\n",
    "                        if j < len(chars) - 1:\n",
    "                            for k, third_char in enumerate(chars[j+1:]):\n",
    "                                if self._is_odia_char(third_char):\n",
    "                                    triple = cluster + '\\u0B4D' + third_char\n",
    "                                    if triple in processed_text:\n",
    "                                        initial_tokens.append(triple)\n",
    "        \n",
    "        # Add common word parts to initial vocabulary\n",
    "        for token in pre_tokens:\n",
    "            if len(token) <= self.max_token_length:\n",
    "                initial_tokens.append(token)\n",
    "        \n",
    "        # Remove duplicates and initialize vocabulary\n",
    "        initial_tokens = list(set(initial_tokens))\n",
    "        vocab_size = len(self.special_tokens) + len(initial_tokens)\n",
    "        self.stoi = {token: i+len(self.special_tokens) for i, token in enumerate(initial_tokens)}\n",
    "        self.itos = {i: token for token, i in self.stoi.items()}\n",
    "        \n",
    "        # Add special tokens\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.stoi[token] = idx\n",
    "            self.itos[idx] = token\n",
    "\n",
    "        # Continue with more aggressive BPE training\n",
    "        ids = [self.stoi[ch] for ch in processed_text]\n",
    "        text_len = len(processed_text)\n",
    "        \n",
    "        while vocab_size < self.max_vocab_size:\n",
    "            # Get pair frequencies\n",
    "            stats = self._get_stats(ids)\n",
    "            if not stats:\n",
    "                break\n",
    "                \n",
    "            # Score all pairs\n",
    "            pair_scores = {\n",
    "                pair: self._get_merge_score(pair, freq, text_len)\n",
    "                for pair, freq in stats.items()\n",
    "            }\n",
    "            \n",
    "            if not pair_scores:\n",
    "                break\n",
    "                \n",
    "            # Select best pair\n",
    "            best_pair = max(pair_scores.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            # Merge tokens\n",
    "            new_token_idx = len(self.stoi)\n",
    "            ids = self._merge(ids, best_pair, new_token_idx)\n",
    "            \n",
    "            # Update vocabularies\n",
    "            self.stoi[best_pair] = new_token_idx\n",
    "            self.itos[new_token_idx] = best_pair\n",
    "            self.merges[best_pair] = new_token_idx\n",
    "            \n",
    "            vocab_size += 1\n",
    "            \n",
    "            # Check compression ratio\n",
    "            current_compression = self._calculate_compression(processed_text, ids)\n",
    "            if current_compression >= self.target_compression:\n",
    "                break\n",
    "                \n",
    "        return self._calculate_compression(processed_text, ids)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Modified encode method with pre-tokenization\"\"\"\n",
    "        if not self.stoi:\n",
    "            raise ValueError(\"Tokenizer needs to be trained first\")\n",
    "            \n",
    "        # Pre-tokenize the text\n",
    "        pre_tokens = self.pre_tokenize(text)\n",
    "        processed_text = ' '.join(pre_tokens)\n",
    "        \n",
    "        # Start with characters\n",
    "        ids = []\n",
    "        for ch in processed_text:\n",
    "            if ch in self.stoi:\n",
    "                ids.append(self.stoi[ch])\n",
    "            else:\n",
    "                ids.append(self.stoi['<UNK>'])\n",
    "                \n",
    "        # Apply merges in order of creation\n",
    "        changes_made = True\n",
    "        while changes_made:\n",
    "            changes_made = False\n",
    "            i = 0\n",
    "            while i < len(ids) - 1:\n",
    "                current_pair = (ids[i], ids[i+1])\n",
    "                if current_pair in self.merges:\n",
    "                    # Replace pair with merged token\n",
    "                    ids[i:i+2] = [self.merges[current_pair]]\n",
    "                    changes_made = True\n",
    "                else:\n",
    "                    i += 1\n",
    "                \n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token indices back to text\n",
    "        \n",
    "        Args:\n",
    "            ids: List of token indices\n",
    "            \n",
    "        Returns:\n",
    "            Decoded text\n",
    "        \"\"\"\n",
    "        text = \"\"\n",
    "        for idx in ids:\n",
    "            if idx in self.itos:\n",
    "                token = self.itos[idx]\n",
    "                if isinstance(token, tuple):\n",
    "                    # Recursively decode merged pairs\n",
    "                    text += self.decode([token[0], token[1]])\n",
    "                else:\n",
    "                    text += token\n",
    "            else:\n",
    "                text += self.itos[self.special_tokens['<UNK>']]\n",
    "        return text\n",
    "\n",
    "    def _is_valid_char(self, char: str) -> bool:\n",
    "        \"\"\"Check if character is valid for tokenization\"\"\"\n",
    "        # Valid Odia range\n",
    "        if '\\u0B00' <= char <= '\\u0B7F':\n",
    "            return True\n",
    "        # Common punctuation and whitespace\n",
    "        if char in {' ', '.', ',', '।', '?', '!', '\\n', '\\t'}:\n",
    "            return True\n",
    "        # Latin characters and numbers\n",
    "        if char.isalnum():\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text from files matching: odia_texts/*.txt\n",
      "\n",
      "Text preview:\n",
      "ସାପ କାମୁଡ଼ା ରୋଗୀଙ୍କ ପାଇଁ ଷ୍ଟ୍ରେଚର ଜେଲ୍ ଫେରିଲେ ଅର୍ଚ୍ଚନା ଅନୁପସ୍ଥିତ ୯୬ ଶିକ୍ଷକ ବହିଷ୍କୃତ ହକି ଷ୍ଟାଡିୟମ ବିମାନବନ୍ଦର ଭିତ୍ତିଭୂମି ସମୀକ୍ଷା କଲେ ୫ ଟି ସଚିବ ଖୁବଶୀଘ୍ର ବିମାନ ସେବା ୧୩ରେ ଭାରତ ସ୍ପେନ୍ ମ୍ୟାଚ୍ ଆପ୍ କବ୍ ଜାରେ ଦିଲ୍ଲୀ ଏମ୍ ସିଡି ବିମୁଦ୍ରାୟନ ମାମଲା ସୁପ୍ରିମକୋର୍ଟରେ ରାୟ ସଂରକ୍ଷିତ ରେପୋରେଟ୍ ବୃଦ୍ଧି କଲା ଆର୍ ବିଆଇ ଇଏମ୍ ଆଇ ବଢ଼ିବ ଛତିଶା ନିଯୋଗ ବୈଠକର ନିଷ୍ପତ୍ତି ଶ୍ରୀମନ୍ଦିରରେ ସ୍ମାର୍ଟଫୋନ ନିଷିଦ୍ଧ\n",
      "\n",
      "ସାପ କାମୁଡ଼ା ରୋଗୀଙ୍କ ପାଇଁ ଷ୍ଟ୍ରେଚର ଜେଲ୍ ଫେରିଲେ ଅର୍ଚ୍ଚନା ଅନୁପସ୍ଥିତ ୯୬ ଶିକ୍ଷକ ବହିଷ୍କୃତ ହକି ଷ୍ଟାଡିୟମ ବିମାନବନ୍ଦର ଭିତ୍ତିଭୂମି ସମୀକ୍ଷା କଲେ ୫ ଟି ସଚିବ ...\n"
     ]
    }
   ],
   "source": [
    "def load_odia_files(file_pattern: str) -> str:\n",
    "    \"\"\"Load all Odia text files matching pattern\"\"\"\n",
    "    text = \"\"\n",
    "    for filename in glob.glob(file_pattern):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            # Clean the text\n",
    "            cleaned = ''.join(char for char in content \n",
    "                            if '\\u0B00' <= char <= '\\u0B7F'  # Odia characters\n",
    "                            or char.isspace()  # Whitespace\n",
    "                            or char in {'.', ',', '।', '?', '!'})  # Punctuation\n",
    "            text += cleaned + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load input files\n",
    "#input_files_pattern = \"data/odia/*.txt\"\n",
    "input_files_pattern = \"odia_texts/*.txt\"\n",
    "try:\n",
    "    text = load_odia_files(input_files_pattern)\n",
    "    print(f\"Loaded text from files matching: {input_files_pattern}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    # Fallback to sample text\n",
    "    text = \"\"\"\n",
    "    ଓଡ଼ିଆ ଭାଷା ଏକ ପ୍ରାଚୀନ ଭାରତୀୟ ଭାଷା।\n",
    "    ଏହା ଭାରତର ଓଡ଼ିଶା ରାଜ୍ୟର ସରକାରୀ ଭାଷା।\n",
    "    \"\"\"\n",
    "    print(\"Using sample text instead\")\n",
    "\n",
    "print(\"\\nText preview:\")\n",
    "print(text[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Achieved compression ratio: 4.00\n",
      "\n",
      "Vocabulary size: 8924\n",
      "Original text length: 11237339\n",
      "Number of tokens: 579\n",
      "\n",
      "Average token length: 4.30 characters\n",
      "Longest token length: 12 characters\n"
     ]
    }
   ],
   "source": [
    "# Create and train tokenizer with enhanced parameters\n",
    "tokenizer = CompressedOdiaTokenizer(\n",
    "    max_vocab_size=16000,        # Increased from 5000\n",
    "    target_compression=4.0,      # Increased from 3.2\n",
    "    max_token_length=24,         # Increased from 12\n",
    "    pattern_type='linguistic'    # Using linguistic patterns\n",
    ")\n",
    "\n",
    "# Train\n",
    "compression = tokenizer.train(text)\n",
    "print(f\"Achieved compression ratio: {compression:.2f}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "tokens = tokenizer.encode(text[:1000])  # Test on first 1000 chars\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nVocabulary size: {len(tokenizer.stoi)}\")\n",
    "print(f\"Original text length: {len(text)}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Calculate token length statistics\n",
    "token_lengths = [len(str(token)) for token in tokenizer.stoi.keys()]\n",
    "avg_len = sum(token_lengths) / len(token_lengths)\n",
    "print(f\"\\nAverage token length: {avg_len:.2f} characters\")\n",
    "print(f\"Longest token length: {max(token_lengths)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Analysis:\n",
      "\n",
      "Token Type Distribution:\n",
      "Odia tokens: 7663\n",
      "Merged tokens: 1257\n",
      "Special tokens: 3\n"
     ]
    }
   ],
   "source": [
    "# Analyze vocabulary composition\n",
    "print(\"Vocabulary Analysis:\")\n",
    "\n",
    "# Count different token types\n",
    "odia_tokens = 0\n",
    "special_tokens = 0\n",
    "merged_tokens = 0\n",
    "\n",
    "for token in tokenizer.stoi.keys():\n",
    "    if isinstance(token, tuple):\n",
    "        merged_tokens += 1\n",
    "    elif token in tokenizer.special_tokens:\n",
    "        special_tokens += 1\n",
    "    elif any('\\u0B00' <= c <= '\\u0B7F' for c in str(token)):\n",
    "        odia_tokens += 1\n",
    "\n",
    "print(f\"\\nToken Type Distribution:\")\n",
    "print(f\"Odia tokens: {odia_tokens}\")\n",
    "print(f\"Merged tokens: {merged_tokens}\")\n",
    "print(f\"Special tokens: {special_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
