{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Byte Pair Encoding for Odia Text\n",
    "\n",
    "## Input Data Structure\n",
    "```\n",
    "project_root/\n",
    "├── odia_bpe_tokenizer_enhanced.ipynb\n",
    "├── data/\n",
    "│   └── odia/\n",
    "│       ├── file1.txt\n",
    "│       └── file2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import regex as re\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Fix for CSS display\n",
    "display(HTML('''\n",
    "<style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_odia_files(file_pattern: str) -> str:\n",
    "    \"\"\"Load all Odia text files matching pattern\"\"\"\n",
    "    text = \"\"\n",
    "    for filename in glob.glob(file_pattern):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            # Clean the text\n",
    "            cleaned = ''.join(char for char in content \n",
    "                            if '\\u0B00' <= char <= '\\u0B7F'  # Odia characters\n",
    "                            or char.isspace()  # Whitespace\n",
    "                            or char in {'.', ',', '।', '?', '!'})  # Punctuation\n",
    "            text += cleaned + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Load input files\n",
    "input_files_pattern = \"data/odia/*.txt\"\n",
    "try:\n",
    "    text = load_odia_files(input_files_pattern)\n",
    "    print(f\"Loaded text from files matching: {input_files_pattern}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading files: {e}\")\n",
    "    # Fallback to sample text\n",
    "    text = \"\"\"\n",
    "    ଓଡ଼ିଆ ଭାଷା ଏକ ପ୍ରାଚୀନ ଭାରତୀୟ ଭାଷା।\n",
    "    ଏହା ଭାରତର ଓଡ଼ିଶା ରାଜ୍ୟର ସରକାରୀ ଭାଷା।\n",
    "    \"\"\"\n",
    "    print(\"Using sample text instead\")\n",
    "\n",
    "print(\"\\nText preview:\")\n",
    "print(text[:500], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train tokenizer with enhanced parameters\n",
    "tokenizer = CompressedOdiaTokenizer(\n",
    "    max_vocab_size=16000,        # Increased from 5000\n",
    "    target_compression=4.0,      # Increased from 3.2\n",
    "    max_token_length=24,         # Increased from 12\n",
    "    pattern_type='linguistic'    # Using linguistic patterns\n",
    ")\n",
    "\n",
    "# Train\n",
    "compression = tokenizer.train(text)\n",
    "print(f\"Achieved compression ratio: {compression:.2f}\")\n",
    "\n",
    "# Test encoding/decoding\n",
    "tokens = tokenizer.encode(text[:1000])  # Test on first 1000 chars\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"\\nVocabulary size: {len(tokenizer.stoi)}\")\n",
    "print(f\"Original text length: {len(text)}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Calculate token length statistics\n",
    "token_lengths = [len(str(token)) for token in tokenizer.stoi.keys()]\n",
    "avg_len = sum(token_lengths) / len(token_lengths)\n",
    "print(f\"\\nAverage token length: {avg_len:.2f} characters\")\n",
    "print(f\"Longest token length: {max(token_lengths)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary composition\n",
    "print(\"Vocabulary Analysis:\")\n",
    "\n",
    "# Count different token types\n",
    "odia_tokens = 0\n",
    "special_tokens = 0\n",
    "merged_tokens = 0\n",
    "\n",
    "for token in tokenizer.stoi.keys():\n",
    "    if isinstance(token, tuple):\n",
    "        merged_tokens += 1\n",
    "    elif token in tokenizer.special_tokens:\n",
    "        special_tokens += 1\n",
    "    elif any('\\u0B00' <= c <= '\\u0B7F' for c in str(token)):\n",
    "        odia_tokens += 1\n",
    "\n",
    "print(f\"\\nToken Type Distribution:\")\n",
    "print(f\"Odia tokens: {odia_tokens}\")\n",
    "print(f\"Merged tokens: {merged_tokens}\")\n",
    "print(f\"Special tokens: {special_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}