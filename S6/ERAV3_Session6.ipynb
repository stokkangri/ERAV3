{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUf77khEnxi9"
      },
      "source": [
        "# Assignment 6 - Part B\n",
        "\n",
        "#### Refer to this code: [COLABLINK](https://colab.research.google.com/drive/1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx)\n",
        "WRITE IT AGAIN SUCH THAT IT ACHIEVES\n",
        "#### 99.4% validation/test accuracy (50/10k split, basically we are calling Validation dataset as test dataset itself)\n",
        "#### Less than 20k Parameters\n",
        "You can use anything from above you want. \n",
        "#### Less than 20 Epochs\n",
        "#### Have used BN, Dropout,\n",
        "#### (Optional): a Fully connected layer or, have used GAP. \n",
        "To learn how to add different things we covered in this session, you can refer to this code: https://www.kaggle.com/enwei26/mnist-digits-pytorch-cnn-99 DONT COPY ARCHITECTURE, JUST LEARN HOW TO INTEGRATE THINGS LIKE DROPOUT, BATCHNORM, ETC.\n",
        "This is a slightly time-consuming assignment, please make sure you start early. You are going to spend a lot of effort running the programs multiple times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [],
      "source": [
        "#Import all necessary libraries\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MDQpaf5obIJ"
      },
      "source": [
        "## Neural network architecture\n",
        "\n",
        "\n",
        "\n",
        "1.   1st Convolution layer with 1 input and 16 output channels\n",
        "2.   This is followed by a batch normalization and a max pool layer\n",
        "3. A dropout layer is added after this\n",
        "4. Another block of exact same layers (convolution, batch norm, max pool and dropout). This has 16 channels as input and 32 channels as output\n",
        "5. Third convolution layer with 32 input channels and 32 output channels. This helps in increasing our receptive field while using small number of parameters\n",
        "6. Final 1x1 convolution to reduce number of channels 32 to 16.\n",
        "7. We use a ReLU activation after each of our convolulation layer\n",
        "8. Finally one fully connected layer that converts 16 channels of 5x5 to 1x10 outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fwwhMKv4a9p9"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.dropout1 = nn.Dropout(0.01)\n",
        "    self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.dropout2 = nn.Dropout(0.01)\n",
        "    self.conv3 = nn.Conv2d(32, 32, kernel_size=3)\n",
        "    self.bn3 = nn.BatchNorm2d(32)\n",
        "    self.dropout3 = nn.Dropout(0.01)\n",
        "    self.conv1x1_reduce = nn.Conv2d(32, 16, kernel_size=1)\n",
        "    self.fc = nn.Linear(16*5*5, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.bn1(self.conv1(x)))  # 26x26x16\n",
        "    x = self.dropout1(x)\n",
        "    x = F.relu(self.bn2(F.max_pool2d(self.conv2(x), 2))) # 12x12x32\n",
        "    x = self.dropout2(x)\n",
        "    x = F.relu(self.bn3(F.max_pool2d(self.conv3(x), 2))) # 5x5x32\n",
        "    x = self.dropout3(x)\n",
        "    #x = F.relu(self.bn4(F.max_pool2d(self.conv4(x), 2))) # 3x3x10\n",
        "    \n",
        "    x = self.conv1x1_reduce(x) # 5x5x16\n",
        "    \n",
        "    x = x.view(-1, 16*5*5)\n",
        "    x = self.fc(x)\n",
        "    x = x.view(-1, 10)\n",
        "    return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdydjYTZFyi3",
        "outputId": "1f4139bd-a289-4adb-f0a0-00ab0e7099b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "       BatchNorm2d-2           [-1, 16, 26, 26]              32\n",
            "           Dropout-3           [-1, 16, 26, 26]               0\n",
            "            Conv2d-4           [-1, 32, 24, 24]           4,640\n",
            "       BatchNorm2d-5           [-1, 32, 12, 12]              64\n",
            "           Dropout-6           [-1, 32, 12, 12]               0\n",
            "            Conv2d-7           [-1, 32, 10, 10]           9,248\n",
            "       BatchNorm2d-8             [-1, 32, 5, 5]              64\n",
            "           Dropout-9             [-1, 32, 5, 5]               0\n",
            "           Conv2d-10             [-1, 16, 5, 5]             528\n",
            "           Linear-11                   [-1, 10]           4,010\n",
            "================================================================\n",
            "Total params: 18,746\n",
            "Trainable params: 18,746\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.50\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 0.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Creating model summary\n",
        "\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available() #Check if CUDA is available or not\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\") #Use CUDA if available\n",
        "model = SimpleCNN().to(device) #Load model to device\n",
        "summary(model, input_size=(1, 28, 28)) #Create summary if input image is 28X28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0XDa4CxsKMC"
      },
      "source": [
        "## Creating Dataloader object for training and testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqTWLaM5GHgH",
        "outputId": "c9977ca0-40c6-42bb-819c-0727f5227e3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset sizes: Full dataset - 60000, Train dataset - 50000, Test dataset - 10000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import torchvision\n",
        "torch.manual_seed(1)\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 128\n",
        "# Load MNIST dataset with augmentations for training\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "\n",
        "                        transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),  # Apply Gaussian Blur\n",
        "                        transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
        "                        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "                        transforms.ToTensor(), \n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Test dataset without augmentations\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),\n",
        "])\n",
        "\n",
        "\n",
        "full_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=None)\n",
        "\n",
        "#full_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "\n",
        "\n",
        "# Step 3: Split the dataset into 50k for training and 10k for testing\n",
        "train_size = 50000\n",
        "test_size = 10000\n",
        "trainset, testset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# Step 4: Create data loaders\n",
        "batch_size = 128\n",
        "\n",
        "# Step 4: Apply specific transforms to the train and test datasets\n",
        "# Wrap the datasets with the appropriate transform\n",
        "trainset.dataset.transform = transform_train\n",
        "testset.dataset.transform = transform_test\n",
        "\n",
        "\n",
        "\n",
        "# Load MNIST training and test datasets\n",
        "#trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(full_dataset , batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Total set of 70K. 60K for training and 10K for testing\n",
        "#testset = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "#testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Check the sizes of the splits\n",
        "print (f\"Dataset sizes: Full dataset - {len(full_dataset)}, Train dataset - {len(trainset)}, Test dataset - {len(testset)}\")\n",
        "#print(f\"Train dataset size: {len(trainset)}\")\n",
        "#print(f\"Test dataset size: {len(testset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdg0f5dqsWD8"
      },
      "source": [
        "## Creating training and testing methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 26, 26]             160\n",
            "       BatchNorm2d-2           [-1, 16, 26, 26]              32\n",
            "           Dropout-3           [-1, 16, 26, 26]               0\n",
            "            Conv2d-4           [-1, 32, 24, 24]           4,640\n",
            "       BatchNorm2d-5           [-1, 32, 12, 12]              64\n",
            "           Dropout-6           [-1, 32, 12, 12]               0\n",
            "            Conv2d-7           [-1, 32, 10, 10]           9,248\n",
            "       BatchNorm2d-8             [-1, 32, 5, 5]              64\n",
            "           Dropout-9             [-1, 32, 5, 5]               0\n",
            "           Conv2d-10             [-1, 16, 5, 5]             528\n",
            "           Linear-11                   [-1, 10]           4,010\n",
            "================================================================\n",
            "Total params: 18,746\n",
            "Trainable params: 18,746\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.50\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 0.57\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# Function to print model summary with layer details\n",
        "def print_model_summary(model, input_size):\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "    summary(model, input_size)\n",
        "\n",
        "print_model_summary(model, (1, 28, 28))  # MNIST images are 1 channel, 28x28 pixels\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() #Set model to train mode\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() #Preventing gradient accumulation\n",
        "        output = model(data)\n",
        "        #loss = criterion(output, target) #Negative log likelihood loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward() #Backpropagation. Weight calculation.\n",
        "        optimizer.step() #Update parameter values\n",
        "        pbar.set_description(desc= f'epoch={epoch} loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval() #Set model to test\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    incorrect_samples = []  # To store incorrect predictions\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            #test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "            # Store incorrect predictions\n",
        "            for i in range(data.size(0)):\n",
        "                if pred[i].item() != target[i].item():\n",
        "                    incorrect_samples.append((data[i].cpu(), target[i].item(), pred[i].item()))\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
        "    \n",
        "    # Display some incorrect predictions\n",
        "    if incorrect_samples and False:\n",
        "        num_display = min(10, len(incorrect_samples))  # Display up to 10 incorrect predictions\n",
        "        fig, axes = plt.subplots(1, num_display, figsize=(15, 3))\n",
        "        \n",
        "        for i, (image, true_label, pred_label) in enumerate(incorrect_samples[:num_display]):\n",
        "            image = image.squeeze(0)  # Remove channel dimension\n",
        "            axes[i].imshow(image, cmap='gray')\n",
        "            axes[i].set_title(f' T: {true_label}, P: {pred_label} ')\n",
        "            axes[i].axis('off')\n",
        "        \n",
        "        plt.show()\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "def train_model(num_epochs=20):\n",
        "\n",
        "    show_images = True\n",
        "    if show_images:\n",
        "        all_images, all_labels = next(iter(torch.utils.data.DataLoader(trainset, batch_size=len(trainset))))\n",
        "        indices = random.sample(range(len(all_images)), 5)\n",
        "\n",
        "        # Step 4: Display the selected images\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, label = all_images[idx], all_labels[idx]\n",
        "            plt.subplot(1, 5, i + 1)\n",
        "            plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "            plt.title(f\"Label: {label.item()}\")\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()  # Show the plot without blocking code execution\n",
        "        #plt.show(block=False)  # Show the plot without blocking code execution\n",
        "        #plt.pause(2)  # Pause for 2 seconds\n",
        "        #plt.close()  # Close the plot window\n",
        "\n",
        "\n",
        "    for epoch in range(0, num_epochs): #Run for 18 epochs\n",
        "        train(model, device, trainloader, optimizer, epoch)\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(testdata=1):\n",
        "    print(f\"Evaluating model: {testdata}\", end=\"\")\n",
        "    if testdata == 1:\n",
        "        return test(model, device, testloader)\n",
        "    else:\n",
        "        return test(model, device, trainloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVwL-dxhsl9L"
      },
      "source": [
        "## Train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMWbLWO6FuHb",
        "outputId": "6f000955-d319-4f3b-db56-a3700732bc86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=0 loss=0.05186476185917854 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.53it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0466, Accuracy: 9846/10000 (98.46%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=1 loss=0.031148681417107582 batch_id=468: 100%|██████████| 469/469 [00:30<00:00, 15.43it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0448, Accuracy: 9847/10000 (98.47%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=2 loss=0.0026431765872985125 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0251, Accuracy: 9910/10000 (99.10%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=3 loss=0.010564235039055347 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.69it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0241, Accuracy: 9910/10000 (99.10%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=4 loss=0.025675460696220398 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.91it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0166, Accuracy: 9939/10000 (99.39%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=5 loss=0.0029225926846265793 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0149, Accuracy: 9948/10000 (99.48%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=6 loss=0.05393640697002411 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.13it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0137, Accuracy: 9951/10000 (99.51%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=7 loss=0.0015578469028696418 batch_id=468: 100%|██████████| 469/469 [00:28<00:00, 16.17it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0159, Accuracy: 9946/10000 (99.46%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=8 loss=0.0068022808991372585 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.93it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0151, Accuracy: 9952/10000 (99.52%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=9 loss=0.0005414550541900098 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.81it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0085, Accuracy: 9971/10000 (99.71%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=10 loss=0.0169388260692358 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.13it/s]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0138, Accuracy: 9944/10000 (99.44%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=11 loss=0.02039267309010029 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.89it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0067, Accuracy: 9977/10000 (99.77%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=12 loss=0.0008886688738130033 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.69it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0100, Accuracy: 9961/10000 (99.61%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=13 loss=0.002259073080495 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 15.99it/s]     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0082, Accuracy: 9971/10000 (99.71%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=14 loss=0.00964026153087616 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.01it/s]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0091, Accuracy: 9966/10000 (99.66%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=15 loss=0.00027339570806361735 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0076, Accuracy: 9980/10000 (99.80%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=16 loss=0.0003780572733376175 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.03it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0063, Accuracy: 9977/10000 (99.77%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=17 loss=0.011560574173927307 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.06it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0089, Accuracy: 9972/10000 (99.72%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=18 loss=0.0005756766768172383 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.09it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0072, Accuracy: 9979/10000 (99.79%)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch=19 loss=4.747032289742492e-05 batch_id=468: 100%|██████████| 469/469 [00:29<00:00, 16.09it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating model: Test accuracy\n",
            "\n",
            "Test set: Average loss: 0.0114, Accuracy: 9959/10000 (99.59%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.15, momentum=0.9) #Using SGD optimizer\n",
        "\n",
        "\n",
        "def train_and_test_model(num_epochs=20):\n",
        "    for epoch in range(0, num_epochs): #Run for 18 epochs\n",
        "        train(model, device, trainloader, optimizer, epoch)\n",
        "        #print(\"Evaluating model: Training accuracy\")\n",
        "        #evaluate_model(testdata=0)\n",
        "        print(\"Evaluating model: Test accuracy\")\n",
        "        test(model, device, testloader)\n",
        "\n",
        "train_and_test_model(num_epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
